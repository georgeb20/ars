{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFLITE_FILENAME = 'efficientdet-lite-colors_final.tflite'\n",
    "LABELS_FILENAME = 'band-labels_colors.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rNiHcfxJc-zj"
   },
   "outputs": [],
   "source": [
    "#using tiling to detect bands instead\n",
    "\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw\n",
    "\n",
    "from pycoral.adapters import common\n",
    "from pycoral.adapters import detect\n",
    "from pycoral.utils.dataset import read_label_file\n",
    "\n",
    "import cv2\n",
    "from color_recognition_api import color_histogram_feature_extraction\n",
    "from color_recognition_api import knn_classifier\n",
    "import os\n",
    "import os.path\n",
    "import sys\n",
    "import string\n",
    "import random\n",
    "\n",
    "Object = collections.namedtuple('Object', ['label', 'score', 'bbox'])\n",
    " \n",
    "  \n",
    "def tiles_location_gen(img_size, tile_size, overlap):\n",
    "  \"\"\"Generates location of tiles after splitting the given image according the tile_size and overlap.\n",
    "\n",
    "  Args:\n",
    "    img_size (int, int): size of original image as width x height.\n",
    "    tile_size (int, int): size of the returned tiles as width x height.\n",
    "    overlap (int): The number of pixels to overlap the tiles.\n",
    "\n",
    "  Yields:\n",
    "    A list of points representing the coordinates of the tile in xmin, ymin,\n",
    "    xmax, ymax.\n",
    "  \"\"\"\n",
    "\n",
    "  tile_width, tile_height = tile_size\n",
    "  img_width, img_height = img_size\n",
    "  h_stride = tile_height - overlap\n",
    "  w_stride = tile_width - overlap\n",
    "  for h in range(0, img_height, h_stride):\n",
    "    for w in range(0, img_width, w_stride):\n",
    "      xmin = w\n",
    "      ymin = h\n",
    "      xmax = min(img_width, w + tile_width)\n",
    "      ymax = min(img_height, h + tile_height)\n",
    "      yield [xmin, ymin, xmax, ymax]\n",
    "\n",
    "\n",
    "def non_max_suppression(objects, threshold):\n",
    "  \"\"\"Returns a list of indexes of objects passing the NMS.\n",
    "\n",
    "  Args:\n",
    "    objects: result candidates.\n",
    "    threshold: the threshold of overlapping IoU to merge the boxes.\n",
    "\n",
    "  Returns:\n",
    "    A list of indexes containings the objects that pass the NMS.\n",
    "  \"\"\"\n",
    "  if len(objects) == 1:\n",
    "    return [0]\n",
    "\n",
    "  boxes = np.array([o.bbox for o in objects])\n",
    "  xmins = boxes[:, 0]\n",
    "  ymins = boxes[:, 1]\n",
    "  xmaxs = boxes[:, 2]\n",
    "  ymaxs = boxes[:, 3]\n",
    "\n",
    "  areas = (xmaxs - xmins) * (ymaxs - ymins)\n",
    "  scores = [o.score for o in objects]\n",
    "  idxs = np.argsort(scores)\n",
    "\n",
    "  selected_idxs = []\n",
    "  while idxs.size != 0:\n",
    "\n",
    "    selected_idx = idxs[-1]\n",
    "    selected_idxs.append(selected_idx)\n",
    "\n",
    "    overlapped_xmins = np.maximum(xmins[selected_idx], xmins[idxs[:-1]])\n",
    "    overlapped_ymins = np.maximum(ymins[selected_idx], ymins[idxs[:-1]])\n",
    "    overlapped_xmaxs = np.minimum(xmaxs[selected_idx], xmaxs[idxs[:-1]])\n",
    "    overlapped_ymaxs = np.minimum(ymaxs[selected_idx], ymaxs[idxs[:-1]])\n",
    "\n",
    "    w = np.maximum(0, overlapped_xmaxs - overlapped_xmins)\n",
    "    h = np.maximum(0, overlapped_ymaxs - overlapped_ymins)\n",
    "\n",
    "    intersections = w * h\n",
    "    unions = areas[idxs[:-1]] + areas[selected_idx] - intersections\n",
    "    ious = intersections / unions\n",
    "\n",
    "    idxs = np.delete(\n",
    "        idxs, np.concatenate(([len(idxs) - 1], np.where(ious > threshold)[0])))\n",
    "\n",
    "\n",
    "  return selected_idxs\n",
    "\n",
    "\n",
    "def draw_object(draw, obj):\n",
    "  \"\"\"Draws detection candidate on the image.\n",
    "\n",
    "  Args:\n",
    "    draw: the PIL.ImageDraw object that draw on the image.\n",
    "    obj: The detection candidate.\n",
    "  \"\"\"\n",
    "  draw.rectangle(obj.bbox, outline='red')\n",
    "  draw.text((obj.bbox[0], obj.bbox[3]), obj.label, fill='#0000')\n",
    "  draw.text((obj.bbox[0], obj.bbox[3] + 10), str(obj.score), fill='#0000')\n",
    "\n",
    "\n",
    "def reposition_bounding_box(bbox, tile_location):\n",
    "  \"\"\"Relocates bbox to the relative location to the original image.\n",
    "\n",
    "  Args:\n",
    "    bbox (int, int, int, int): bounding box relative to tile_location as xmin,\n",
    "      ymin, xmax, ymax.\n",
    "    tile_location (int, int, int, int): tile_location in the original image as\n",
    "      xmin, ymin, xmax, ymax.\n",
    "\n",
    "  Returns:\n",
    "    A list of points representing the location of the bounding box relative to\n",
    "    the original image as xmin, ymin, xmax, ymax.\n",
    "  \"\"\"\n",
    "  bbox[0] = bbox[0] + tile_location[0]\n",
    "  bbox[1] = bbox[1] + tile_location[1]\n",
    "  bbox[2] = bbox[2] + tile_location[0]\n",
    "  bbox[3] = bbox[3] + tile_location[1]\n",
    "  return bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tflite_runtime.interpreter as tflite \n",
    "import multiprocessing\n",
    "\n",
    "interpreter = tflite.Interpreter(TFLITE_FILENAME,num_threads=multiprocessing.cpu_count())\n",
    "interpreter.allocate_tensors()\n",
    "labels = read_label_file(LABELS_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "tFbgOuFDdGOq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80x60\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\georg\\AppData\\Local\\Temp\\ipykernel_16604\\4170727870.py:62: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  lambda size, img=tile: img.resize(size, Image.NEAREST))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\georg\\OneDrive\\Desktop\\band\\band\\tiling_inference.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 54>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/georg/OneDrive/Desktop/band/band/tiling_inference.ipynb#W3sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m tile \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39mcrop(tile_location)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/georg/OneDrive/Desktop/band/band/tiling_inference.ipynb#W3sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m _, scale \u001b[39m=\u001b[39m common\u001b[39m.\u001b[39mset_resized_input(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/georg/OneDrive/Desktop/band/band/tiling_inference.ipynb#W3sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m     interpreter, tile\u001b[39m.\u001b[39msize,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/georg/OneDrive/Desktop/band/band/tiling_inference.ipynb#W3sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m     \u001b[39mlambda\u001b[39;00m size, img\u001b[39m=\u001b[39mtile: img\u001b[39m.\u001b[39mresize(size, Image\u001b[39m.\u001b[39mNEAREST))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/georg/OneDrive/Desktop/band/band/tiling_inference.ipynb#W3sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m interpreter\u001b[39m.\u001b[39;49minvoke()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/georg/OneDrive/Desktop/band/band/tiling_inference.ipynb#W3sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m objs \u001b[39m=\u001b[39m detect\u001b[39m.\u001b[39mget_objects(interpreter, score_threshold, scale)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/georg/OneDrive/Desktop/band/band/tiling_inference.ipynb#W3sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m objs:\n",
      "File \u001b[1;32mc:\\Users\\georg\\anaconda3\\envs\\band\\lib\\site-packages\\tflite_runtime\\interpreter.py:833\u001b[0m, in \u001b[0;36mInterpreter.invoke\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    821\u001b[0m \u001b[39m\"\"\"Invoke the interpreter.\u001b[39;00m\n\u001b[0;32m    822\u001b[0m \n\u001b[0;32m    823\u001b[0m \u001b[39mBe sure to set the input sizes, allocate tensors and fill values before\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    830\u001b[0m \u001b[39m  ValueError: When the underlying interpreter fails raise ValueError.\u001b[39;00m\n\u001b[0;32m    831\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    832\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_safe()\n\u001b[1;32m--> 833\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpreter\u001b[39m.\u001b[39;49mInvoke()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "#using tiling to detect bands instead\n",
    "    #ARGS HERE!!!\n",
    "\n",
    "def sort_contours(cnts, method=\"top-to-bottom\"):\n",
    "    # initialize the reverse flag and sort index\n",
    "    reverse = False\n",
    "    i = 0\n",
    "\n",
    "    # handle if we need to sort in reverse\n",
    "    if method == \"right-to-left\" or method == \"bottom-to-top\":\n",
    "        reverse = True\n",
    "\n",
    "    # handle if we are sorting against the y-coordinate rather than\n",
    "    # the x-coordinate of the bounding box\n",
    "    if method == \"top-to-bottom\" or method == \"bottom-to-top\":\n",
    "        i = 1\n",
    "\n",
    "    # construct the list of bounding boxes and sort them from top to\n",
    "    # bottom\n",
    "    boundingBoxes = [c.bbox for c in cnts]\n",
    "    (cnts, boundingBoxes) = zip(*sorted(zip(cnts, boundingBoxes),\n",
    "        key=lambda b:b[1][i], reverse=reverse))\n",
    "\n",
    "    # return the list of sorted contours and bounding boxes\n",
    "    return (cnts, boundingBoxes)\n",
    "\n",
    "img_path = 'Resistor_Lab_46.jpg'\n",
    "\n",
    "\n",
    "tile_overlap = 15\n",
    "#tile_sizes = \"160x120\"\n",
    "score_threshold = .2\n",
    "iou_threshold = .3\n",
    "#ARGS ENDED HERE!!!\n",
    "\n",
    "\n",
    "# Open image.\n",
    "image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "draw = ImageDraw.Draw(image)\n",
    "\n",
    "objects_by_label = dict()\n",
    "img_size = image.size\n",
    "x_tile = int(img_size[0]/2)\n",
    "y_tile = int(img_size[1]/2)\n",
    "tile_sizes = str(x_tile)+\"x\"+str(y_tile)\n",
    "print(tile_sizes)\n",
    "tile_sizes = [\n",
    "    map(int, tile_size.split('x')) for tile_size in tile_sizes.split(',')\n",
    "]\n",
    "\n",
    "image2 = Image.open(img_path)\n",
    "\n",
    "for tile_size in tile_sizes:\n",
    "\n",
    "  for tile_location in tiles_location_gen(img_size, tile_size,\n",
    "                                          tile_overlap):\n",
    "    print(1)\n",
    "    tile = image.crop(tile_location)\n",
    "    _, scale = common.set_resized_input(\n",
    "        interpreter, tile.size,\n",
    "        lambda size, img=tile: img.resize(size, Image.NEAREST))\n",
    "    interpreter.invoke()\n",
    "    objs = detect.get_objects(interpreter, score_threshold, scale)\n",
    "    for obj in objs:\n",
    "      bbox = [obj.bbox.xmin, obj.bbox.ymin, obj.bbox.xmax, obj.bbox.ymax]\n",
    "      bbox = reposition_bounding_box(bbox, tile_location)\n",
    "      label = labels.get(obj.id, '')\n",
    "      print(label)\n",
    "      objects_by_label.setdefault(label,\n",
    "                                  []).append(Object(label, obj.score, bbox))\n",
    "bands = []\n",
    "letters = string.ascii_lowercase\n",
    "new_objs=[]\n",
    "\n",
    "for label, objects in objects_by_label.items():\n",
    "  idxs = non_max_suppression(objects, iou_threshold)\n",
    "  for idx in idxs:\n",
    "   # bbox = objects[idx].bbox\n",
    "    new_objs.append(objects[idx])\n",
    "\n",
    "objects=sort_contours(new_objs)\n",
    "for label, objects in objects_by_label.items():\n",
    "  #idxs = non_max_suppression(objects, iou_threshold)\n",
    "  print(idxs)\n",
    "  #for idx in idxs:\n",
    "  for idx in range(len(objects)):\n",
    "    draw_object(draw, objects[idx])\n",
    "\n",
    "    bbox = objects[idx].bbox\n",
    "\n",
    "\n",
    "    img_crop = image2.crop(box=bbox)\n",
    "    strs = ''.join(random.choice(letters) for i in range(10))\n",
    "    strs = strs+\".jpg\"\n",
    "    #img_crop.save(strs)\n",
    "\n",
    "    opencvImage = cv2.cvtColor(np.array(img_crop), cv2.COLOR_RGB2BGR)\n",
    "      # checking whether the training data is ready\n",
    "    \n",
    "          \n",
    "\n",
    "    # get the prediction\n",
    "    color_histogram_feature_extraction.color_histogram_of_test_image(opencvImage)\n",
    "    prediction = knn_classifier.main('training.data', 'test.data')\n",
    "    print('Detected color is:', prediction)\n",
    "    bands.append(prediction)\n",
    "\n",
    "\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACIAAAAKCAIAAABe2hvOAAACkklEQVR4nE3PvW4kRRQF4HNuVff0/HlsbITYABEQ88IEJPsaPAgBGywIhHY8lu3p6eqquucSICS+N/j48eefLMlCiF5bkddWltpukFtidF/XJXNX203RaB3ovXcJYN6Md62yVUkgyQwyaHFp2/gPyXGccmJAIfVQRa/hq3xBr2SMZhiRUq4LAJNEKMJdTQ6Cr/NLTtucp2SQpNaD3Qy0CRGICAeACM8kw129hdfeK6IbIplytnEwUNZZ1wQyYFIo3N3lIHtZ6yaTYSklSIgWcgUsW0RIAj3CIiIb6IAkucuboRNKiTmZJVARiFprKaW2RVEUa29riOSQOLp70S1EgwDRRAYn4n8iIs8oY9xyfeb1/PWAy19/3B9Otck290Xj4eHb88vb7vdf0roOtUmSUJtLIKx3bHeHda1D3pjlMN4d7909+q+b7VG2u66YlWOT80Cz+HcTb8t7KfXz5fP55bo9PH73w4/jNDnen58vpZTWPMQu710hkMllS2neY3sgoq/ruhaPiKxp02AjGsZAIpmToLXVpWqp8/mS1N6v8+1WZOV4ui+1/fbp054DDbQGIDMHFKJZLrclqox52uwEXueylDWloVxvTWkTo6fMgQAyai/zcnu9+jy/vb7thlSqpv3dZn883j/8eX77+8v5G3itvTUBiKB3CDBiGA+wnIdxf3qU4/J660iWxq4lyZPcMklKyFq9zGWeFy11nA632/v7su6POxs3lUzb7eOHD3Y+IxLgAAijBUGzbJZhOQ1DmvZ9ratHuDo9j6kL5pFBBd09S+pCIDNPD1+dXr7wkNL+9DQdH661euLp6ekwfV9rXZsDiID3iAimXNeeUrY0DIdjs2LbIwAOg6UisLnCO+g9+j9EyA5T7tQsoQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=34x10>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=3\n",
    "#print(bands[i])\n",
    "gimg[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "BXsNK7YVdYLk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['black', 'red', 'gold', 'brown']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "bands.reverse()\n",
    "colors = [\"black\",\"brown\",\"red\",\"orange\",\"yellow\",\"green\",\"blue\",\"violet\",\"gold\"]\n",
    "values = [0,1,2,3,4,5,6,7,-1]\n",
    "print(bands)\n",
    "def color2res(bands,colors,values):\n",
    "    flag=0\n",
    "    if bands == []:\n",
    "        pass\n",
    "    else:\n",
    "        if(bands[0]==\"Gold\"):\n",
    "          bands.reverse()\n",
    "          flag=1  \n",
    "        if(len(bands)==4):\n",
    "            resistance =  (values[colors.index(bands[0])]*10 + values[colors.index(bands[1])]) * pow(10,(values[colors.index(bands[2])]))\n",
    "        else:\n",
    "            resistance =  (values[colors.index(bands[0])]*100 + values[colors.index(bands[1])]*10+values[colors.index(bands[2])]) * pow(10,(values[colors.index(bands[2])]))\n",
    "\n",
    "        if flag==1:\n",
    "          bands.reverse()\n",
    "        return resistance\n",
    "  \n",
    "resistance = color2res(bands,colors,values)\n",
    "resistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "2dJ0I79BdZpx"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'resistance' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\georg\\OneDrive\\Desktop\\band\\band\\tiling_inference.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/georg/OneDrive/Desktop/band/band/tiling_inference.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m height_ratio \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39mheight \u001b[39m/\u001b[39m image\u001b[39m.\u001b[39mwidth\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/georg/OneDrive/Desktop/band/band/tiling_inference.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m#image = image.resize((display_width, int(display_width * height_ratio)))\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/georg/OneDrive/Desktop/band/band/tiling_inference.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m#draw_objects(ImageDraw.Draw(image), objs, scale_factor, labels)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/georg/OneDrive/Desktop/band/band/tiling_inference.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/georg/OneDrive/Desktop/band/band/tiling_inference.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m#draw = ImageDraw.Draw(image) \u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/georg/OneDrive/Desktop/band/band/tiling_inference.ipynb#W5sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m text \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(resistance)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/georg/OneDrive/Desktop/band/band/tiling_inference.ipynb#W5sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# drawing text size\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/georg/OneDrive/Desktop/band/band/tiling_inference.ipynb#W5sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m draw\u001b[39m.\u001b[39mtext((\u001b[39m5\u001b[39m, \u001b[39m5\u001b[39m), text, align \u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m\"\u001b[39m) \n",
      "\u001b[1;31mNameError\u001b[0m: name 'resistance' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "display_width = 500\n",
    "scale_factor = display_width / image.width\n",
    "height_ratio = image.height / image.width\n",
    "#image = image.resize((display_width, int(display_width * height_ratio)))\n",
    "#draw_objects(ImageDraw.Draw(image), objs, scale_factor, labels)\n",
    "\n",
    "#draw = ImageDraw.Draw(image) \n",
    "  \n",
    "  \n",
    "text = str(resistance)\n",
    "  \n",
    "# drawing text size\n",
    "draw.text((5, 5), text, align =\"left\") \n",
    "\n",
    "print(bands)\n",
    "print(text)\n",
    "image_original = Image.open(img_path)\n",
    "\n",
    "f = plt.figure()\n",
    "f.add_subplot(1,2, 1)\n",
    "plt.imshow(np.rot90(image_original,2))\n",
    "f.add_subplot(1,2, 2)\n",
    "plt.imshow(np.rot90(image,2))\n",
    "plt.show(block=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lsn8dqNadVdq"
   },
   "outputs": [],
   "source": [
    "#!pip install opencv-python-headless==4.1.2.30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "cWZYqZRcdWvh"
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 0-dimensional, but 3 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\georg\\OneDrive\\Desktop\\band\\band\\tiling_inference.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/georg/OneDrive/Desktop/band/band/tiling_inference.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m   open_cv_image \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(i) \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/georg/OneDrive/Desktop/band/band/tiling_inference.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m   \u001b[39m# Convert RGB to BGR \u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/georg/OneDrive/Desktop/band/band/tiling_inference.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m   open_cv_bands\u001b[39m.\u001b[39mappend(open_cv_image[:, :, ::\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39mcopy())\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/georg/OneDrive/Desktop/band/band/tiling_inference.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m#ur script is below\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/georg/OneDrive/Desktop/band/band/tiling_inference.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/georg/OneDrive/Desktop/band/band/tiling_inference.ipynb#X10sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m#\"Black\",\"Brown\",\"Grey\",\"White\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/georg/OneDrive/Desktop/band/band/tiling_inference.ipynb#X10sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m colors \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mBlack\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mBrown\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mRed\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mOrange\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mYellow\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mGreen\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mBlue\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mViolet\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mGold\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array: array is 0-dimensional, but 3 were indexed"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "open_cv_bands=[]\n",
    "for i in bands:\n",
    "  open_cv_image = np.array(i) \n",
    "  # Convert RGB to BGR \n",
    "  open_cv_bands.append(open_cv_image[:, :, ::-1].copy())\n",
    "\n",
    "#ur script is below\n",
    "\n",
    "#\"Black\",\"Brown\",\"Grey\",\"White\"\n",
    "colors = [\"Black\",\"Brown\",\"Red\",\"Orange\",\"Yellow\",\"Green\",\"Blue\",\"Violet\",\"Gold\"]\n",
    "values = [0,1,2,3,4,5,6,7,-1]\n",
    "all_lower = np.array([[0,22,13],[5,40,48],[0,100,100],[10,150,60],[30,90,45],[40,110,40],[94,80,2],[165,150,65],[13,125,95]])\n",
    "all_higher = np.array([[50,255,40],[23,255,90],[10,255,255],[20,180,100],[50,255,255],[60,150,65],[126,255,255],[180,255,255],[18,150,150]])\n",
    "\n",
    "final_colors=[]\n",
    "for i in open_cv_bands:\n",
    "  band_image = np.array(i)\n",
    "  #result=image.copy()\n",
    "  #cv2.imshow(\"Original\", image)\n",
    "    \n",
    "  hsv = cv2.cvtColor(band_image, cv2.COLOR_BGR2HSV)\n",
    "  \n",
    "  max=0\n",
    "  save_i=0\n",
    "  for i in range(len(colors)):\n",
    "      # lower boundary RED color range values; Hue (0 - 10)\n",
    "      lower = all_lower[i]\n",
    "      upper = all_higher[i]\n",
    "\n",
    "      \n",
    "      mask = cv2.inRange(hsv, lower, upper)\n",
    "      if(i==0):\n",
    "          save_mask=mask.copy()\n",
    "      #result = cv2.bitwise_and(image, image, mask=mask)\n",
    "      if(np.sum(mask)>max):\n",
    "          max=np.sum(mask)\n",
    "          save_i=i\n",
    "          save_mask=mask.copy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  final_colors.append(colors[save_i])\n",
    "\n",
    "  #result = cv2.bitwise_and(result, result, mask=save_mask)\n",
    "  # cv2.imshow('mask', save_mask)\n",
    "  # cv2.imshow('result',result)\n",
    "\n",
    "  # print(max)\n",
    "  # cv2.waitKey(0)\n",
    "  # cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "tiling_inference.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('band')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "693cdbb358a2bdc40ce36207696ed63be159f188f11a99bdc0604f8b89b80557"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
